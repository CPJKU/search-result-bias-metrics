{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Bias Measurement of Information Retrieval\n",
    "https://arxiv.org/abs/2005.00372"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: calculate_bias_documents\n",
    "Step 1 calculates the gender bias of each document in collection. Set `collection_path` to the path of the collection with TSV format. For MS MARCO collection, this should refer to the `collection.tsv` file. The code does not apply any particular pre-processing to the text unless converting it to lower case.\n",
    "* Input: collection.tsv, wordlist_genderspecific.txt\n",
    "* Output: docs_bias_bool.pkl, docs_bias_tc.pkl, docs_bias_tf.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import copy\n",
    "from csv import DictWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_directory = os.path.dirname(os.getcwd())\n",
    "path_collection = os.path.join(path_directory, 'MSMARCO', 'collection.tsv')\n",
    "path_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = path_collection #path to a preprocessed version of collection.tsv provided by MSMARCO collection\n",
    "\n",
    "wordlist_path = \"resources/wordlist_genderspecific.txt\"\n",
    "\n",
    "if not os.path.exists(\"data\") or not os.path.isdir(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "docs_bias_save_paths = {'tc':\"data/docs_bias_tc.pkl\",\n",
    "                        'bool':\"data/docs_bias_bool.pkl\",\n",
    "                        'tf':\"data/docs_bias_tf.pkl\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of gender-related words\n",
    "genderwords_feml = []\n",
    "genderwords_male = []\n",
    "\n",
    "for l in open(wordlist_path):\n",
    "    vals = l.strip().lower().split(',')\n",
    "    if vals[1]=='f':\n",
    "        genderwords_feml.append(vals[0])\n",
    "    elif vals[1]=='m':\n",
    "        genderwords_male.append(vals[0])\n",
    "\n",
    "genderwords_feml = set(genderwords_feml)\n",
    "genderwords_male = set(genderwords_male)\n",
    "\n",
    "print(len(genderwords_feml), len(genderwords_male))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to convert to lower case and get gender magnitudes: term count, term frequency and boolean\n",
    "def get_tokens(text):\n",
    "    return text.lower().split(\" \")\n",
    "\n",
    "def get_bias(tokens):\n",
    "    text_cnt = collections.Counter(tokens)\n",
    "    \n",
    "    cnt_feml = 0\n",
    "    cnt_male = 0\n",
    "    cnt_logfeml = 0\n",
    "    cnt_logmale = 0\n",
    "    for word in text_cnt:\n",
    "        if word in genderwords_feml:\n",
    "            cnt_feml += text_cnt[word]\n",
    "            cnt_logfeml += np.log(text_cnt[word] + 1)\n",
    "        elif word in genderwords_male:\n",
    "            cnt_male += text_cnt[word]\n",
    "            cnt_logmale += np.log(text_cnt[word] + 1)\n",
    "    text_len = np.sum(list(text_cnt.values()))\n",
    "    \n",
    "    bias_tc = (float(cnt_feml - cnt_male), float(cnt_feml), float(cnt_male))\n",
    "    bias_tf = (np.log(cnt_feml + 1) - np.log(cnt_male + 1), np.log(cnt_feml + 1), np.log(cnt_male + 1))\n",
    "    bias_bool = (np.sign(cnt_feml) - np.sign(cnt_male), np.sign(cnt_feml), np.sign(cnt_male))\n",
    "    \n",
    "    return bias_tc, bias_tf, bias_bool\n",
    "\n",
    "get_bias (get_tokens(\"a war day and many boys , women and men\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender magnitude for all documents in collection\n",
    "docs_bias = {'tc':{}, 'tf':{}, 'bool':{}}\n",
    "empty_cnt = 0\n",
    "with open(collection_path) as fr:\n",
    "    for i, line in enumerate(fr):\n",
    "        vals = line.strip().split('\\t')\n",
    "        docid = int(vals[0])\n",
    "        if len(vals) == 2:\n",
    "            _text = vals[1]\n",
    "        else:\n",
    "            _text = \"\"\n",
    "            empty_cnt += 1\n",
    "        \n",
    "        _res = get_bias(get_tokens(_text))\n",
    "        docs_bias['tc'][docid] = _res[0]\n",
    "        docs_bias['tf'][docid] = _res[1]\n",
    "        docs_bias['bool'][docid] = _res[2]\n",
    "            \n",
    "        if i % 1000000 == 0:\n",
    "            print (i)\n",
    "            \n",
    "print ('done!')\n",
    "print ('number of skipped documents: %d' % empty_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving bias values of documents\n",
    "for _method in docs_bias:\n",
    "    print (_method)\n",
    "    with open(docs_bias_save_paths[_method], 'wb') as fw:\n",
    "        pickle.dump(docs_bias[_method], fw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: calculate_bias_runs\n",
    "Using the pre-calculated document biases, Step 2 calculates gender bias scores for each query for the given retrieval run files.\n",
    "* Inputs: .run files for each model, docs_bias_tc/tf/bool.pkl (from step 1), queries_gender_annotated.csv\n",
    "* Outputs: e.g.: run_bias_bi_msmarco_L6_bool_ARaB.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to TREC run files\n",
    "experiments_paths = {'bi_msmarco_L6': 'reranker_trec_run/results_bi_msmarco_L6.run',\n",
    "                   'bi_msmarco_L12': 'reranker_trec_run/results_bi_msmarco_L12.run',\n",
    "                   'cross_msmarco_L6': 'reranker_trec_run/results_cross_msmarco_L6.run',\n",
    "                   'cross_msmarco_L12': 'reranker_trec_run/results_cross_msmarco_L12.run'}\n",
    "\n",
    "docs_bias_paths = {'tc':\"data/docs_bias_tc.pkl\",\n",
    "                   'tf':\"data/docs_bias_tf.pkl\",\n",
    "                   'bool':\"data/docs_bias_bool.pkl\",\n",
    "                   }\n",
    "\n",
    "at_ranklist = [5, 10, 20, 30, 40]\n",
    "\n",
    "queries_gender_annotated_path = \"resources/queries_gender_annotated.csv\"\n",
    "\n",
    "save_path_base = \"data/all\"\n",
    "if not os.path.exists(save_path_base):\n",
    "    os.makedirs(save_path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading saved document bias values\n",
    "docs_bias = {}\n",
    "for _method in docs_bias_paths:\n",
    "    print (_method)\n",
    "    with open(docs_bias_paths[_method], 'rb') as fr:\n",
    "        docs_bias[_method] = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading gendered queries (in our case all)\n",
    "qryids_filter = []\n",
    "with open(queries_gender_annotated_path, 'r') as fr:\n",
    "    for line in fr:\n",
    "        vals = line.strip().split(',')\n",
    "        qryid = vals[0]\n",
    "        qryids_filter.append(qryid)\n",
    "\n",
    "qryids_filter = set(qryids_filter)\n",
    "print (len(qryids_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading run files\n",
    "\n",
    "runs_docs_bias = {}\n",
    "    \n",
    "for exp_name in experiments_paths:\n",
    "    \n",
    "    run_path = experiments_paths[exp_name]\n",
    "    runs_docs_bias[exp_name] = {}\n",
    "    \n",
    "    for _method in docs_bias_paths:\n",
    "        runs_docs_bias[exp_name][_method] = {}\n",
    "    \n",
    "    with open(run_path) as fr:\n",
    "        qryid_cur = 0\n",
    "        for i, line in enumerate(fr):\n",
    "            vals = line.strip().split(' ')\n",
    "            if len(vals) == 6:\n",
    "                qryid = vals[0] #int(vals[0])\n",
    "                docid = int(vals[2])\n",
    "\n",
    "                if (qryid not in qryids_filter):\n",
    "                    continue\n",
    "                \n",
    "                if qryid != qryid_cur:\n",
    "                    for _method in docs_bias_paths:\n",
    "                        runs_docs_bias[exp_name][_method][qryid] = []\n",
    "                    qryid_cur = qryid\n",
    "                for _method in docs_bias_paths:\n",
    "                    runs_docs_bias[exp_name][_method][qryid].append(docs_bias[_method][docid])\n",
    "      \n",
    "    for _method in docs_bias_paths:\n",
    "        print (\"Number of effective queries in %s using %s : %d\" % (exp_name, _method, len(runs_docs_bias[exp_name][_method].keys())))\n",
    "    print ()\n",
    "print ('done!')\n",
    "\n",
    "runs_docs_bias_FM = copy.deepcopy(runs_docs_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_docs_bias_FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_RaB_q(bias_list, at_rank):\n",
    "    bias_val = np.mean([x[0] for x in bias_list[:at_rank]])\n",
    "    bias_feml_val = np.mean([x[1] for x in bias_list[:at_rank]])\n",
    "    bias_male_val = np.mean([x[2] for x in bias_list[:at_rank]])\n",
    "    \n",
    "    return bias_val, bias_feml_val, bias_male_val\n",
    "       \n",
    "    \n",
    "def calc_ARaB_q(bias_list, at_rank):\n",
    "    \n",
    "    _vals = []\n",
    "    _feml_vals = []\n",
    "    _male_vals = []\n",
    "    for t in range(at_rank):\n",
    "        if len(bias_list) >= t+1:\n",
    "            _val_RaB, _feml_val_RaB, _male_val_RaB = calc_RaB_q(bias_list, t+1)\n",
    "            _vals.append(_val_RaB)\n",
    "            _feml_vals.append(_feml_val_RaB)\n",
    "            _male_vals.append(_male_val_RaB)\n",
    "\n",
    "    bias_val = np.mean(_vals)\n",
    "    bias_feml_val = np.mean(_feml_vals)\n",
    "    bias_male_val = np.mean(_male_vals)\n",
    "    \n",
    "    return bias_val, bias_feml_val, bias_male_val\n",
    "\n",
    "def calc_nDRaB_q(bias_list, at_rank):\n",
    "    weight = 1/np.log2(np.arange(1,at_rank+1)+1)\n",
    "    \n",
    "    bias_val = np.mean([x[0] for x in bias_list[:at_rank]]*weight)/np.mean(weight)\n",
    "    bias_feml_val = np.mean([x[1] for x in bias_list[:at_rank]]*weight)/np.mean(weight)\n",
    "    bias_male_val = np.mean([x[2] for x in bias_list[:at_rank]]*weight)/np.mean(weight)\n",
    "    \n",
    "    return bias_val, bias_feml_val, bias_male_val\n",
    "\n",
    "_test = [(0.0, 0.0, 0.0),(3, 3, 0.0),(0, 0, 0.0),(0, 0, 0.0),(0, 0, 0.0),(0, 0, 0.0),(0, 0.0, 0.0),(-5, 0.0, 5),(0, 0.0, 0.0),(-2, 0.0, 2)]\n",
    "#_test = [(10, 0, 0),(1, 1, 0),(0, 0, 0),(1, 1, 0),(0, 0, 0),(-1, 1, 0),(-1, 1, 0),(0, 0, 0),(-1, 1, 0),(1, 1, 0)]\n",
    "\n",
    "print ('RaB_q', calc_RaB_q(_test, 10))\n",
    "print ('ARaB_q', calc_ARaB_q(_test, 10))\n",
    "print ('nDRaB_q', calc_nDRaB_q(_test, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_bias_RaB = {}\n",
    "qry_bias_ARaB = {}\n",
    "qry_bias_nDRaB = {}\n",
    "     \n",
    "print ('Calculating ranking bias ...')\n",
    "\n",
    "        \n",
    "for exp_name in experiments_paths:\n",
    "    qry_bias_RaB[exp_name] = {}\n",
    "    qry_bias_ARaB[exp_name] = {}\n",
    "    qry_bias_nDRaB[exp_name] = {}\n",
    "\n",
    "\n",
    "    for _method in docs_bias_paths:\n",
    "        print (exp_name, _method)\n",
    "\n",
    "        qry_bias_RaB[exp_name][_method] = {}\n",
    "        qry_bias_ARaB[exp_name][_method] = {}\n",
    "        qry_bias_nDRaB[exp_name][_method] = {}\n",
    "\n",
    "        for at_rank in at_ranklist:\n",
    "            qry_bias_RaB[exp_name][_method][at_rank] = {}\n",
    "            qry_bias_ARaB[exp_name][_method][at_rank] = {}\n",
    "            qry_bias_nDRaB[exp_name][_method][at_rank] = {}\n",
    "\n",
    "            for qry_id in runs_docs_bias[exp_name][_method]:\n",
    "                qry_bias_RaB[exp_name][_method][at_rank][qry_id] = calc_RaB_q(runs_docs_bias[exp_name][_method][qry_id], at_rank)\n",
    "                qry_bias_ARaB[exp_name][_method][at_rank][qry_id] = calc_ARaB_q(runs_docs_bias[exp_name][_method][qry_id], at_rank)\n",
    "                qry_bias_nDRaB[exp_name][_method][at_rank][qry_id] = calc_nDRaB_q(runs_docs_bias[exp_name][_method][qry_id], at_rank)\n",
    "    \n",
    "print ('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in experiments_paths:\n",
    "    for _method in docs_bias_paths:\n",
    "        save_path = save_path_base + \"/run_bias_%s_%s\" % (exp_name, _method)\n",
    "\n",
    "        print (save_path)\n",
    "\n",
    "        with open(save_path + '_RaB.pkl', 'wb') as fw:\n",
    "            pickle.dump(qry_bias_RaB[exp_name][_method], fw)\n",
    "\n",
    "        with open(save_path + '_ARaB.pkl', 'wb') as fw:\n",
    "            pickle.dump(qry_bias_ARaB[exp_name][_method], fw)\n",
    "            \n",
    "        with open(save_path + '_nDRaB.pkl', 'wb') as fw:\n",
    "            pickle.dump(qry_bias_nDRaB[exp_name][_method], fw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: bias_metrics\n",
    "Step 3 calculates the final gender bias metrics for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list(experiments_paths.keys())\n",
    "\n",
    "metrics = ['RaB', 'ARaB', 'nDRaB'] \n",
    "methods = ['tf', 'bool']\n",
    "\n",
    "\n",
    "qry_bias_paths = {}\n",
    "for metric in metrics:\n",
    "    qry_bias_paths[metric] = {}\n",
    "    for exp_name in experiments:\n",
    "        qry_bias_paths[metric][exp_name] = {}\n",
    "        for _method in methods:\n",
    "            qry_bias_paths[metric][exp_name][_method] = save_path_base + '/run_bias_%s_%s_%s.pkl' % (exp_name, _method, metric)\n",
    "\n",
    "results_path = 'results'\n",
    "if not os.path.exists(results_path) or not os.path.isdir(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_bias_perqry = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    qry_bias_perqry[metric] = {}\n",
    "    for exp_name in experiments:\n",
    "        qry_bias_perqry[metric][exp_name] = {}\n",
    "        for _method in methods:\n",
    "            _path = qry_bias_paths[metric][exp_name][_method]\n",
    "            print (_path)\n",
    "            with open(_path, 'rb') as fr:\n",
    "                qry_bias_perqry[metric][exp_name][_method] = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_effective = {}\n",
    "with open(queries_gender_annotated_path, 'r') as fr:\n",
    "    for line in fr:\n",
    "        vals = line.strip().split(',')\n",
    "        qryid = vals[0] #int(vals[0])\n",
    "        qrytext = ' '.join(vals[1:-1])\n",
    "        qrygender = vals[-1]\n",
    "        if qrygender == 'n':\n",
    "            queries_effective[qryid] = qrytext\n",
    "len(queries_effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results per query (F, M)\n",
    "results = []\n",
    "\n",
    "for at_rank in at_ranklist:\n",
    "    for _method in methods:\n",
    "        for exp_name in experiments:\n",
    "            for metric in metrics:\n",
    "                for qryid in queries_effective.keys():\n",
    "                    results.append(dict(\n",
    "                        rank=at_rank,\n",
    "                        method=_method,\n",
    "                        model=exp_name,\n",
    "                        metric=metric,\n",
    "                        query = qryid,\n",
    "                        F=qry_bias_perqry[metric][exp_name][_method][at_rank][qryid][1],\n",
    "                        M=qry_bias_perqry[metric][exp_name][_method][at_rank][qryid][2]))\n",
    "\n",
    "path = os.path.join(results_path, 'FM_per_query' + '.csv')\n",
    "\n",
    "with open(path, 'w+') as f:\n",
    "    writer = DictWriter(f, fieldnames=results[0].keys(), delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results and save in needed format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(path, sep='\\t')\n",
    "\n",
    "df['P'] = np.where(df['query'].astype(str).str[2]=='F', df['F'], df['M'])\n",
    "df['CP'] = np.where(df['query'].astype(str).str[2]=='F', df['M'], df['F'])\n",
    "df['P-CP'] = df['P'] - df['CP']\n",
    "df.drop(['F','M'], axis=1, inplace=True)\n",
    "\n",
    "# save per query results (P, CP, P-CP)\n",
    "path_per_query = os.path.join(results_path, 'res_per_query' + '.csv')\n",
    "df.to_csv(path_per_query, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per topic\n",
    "df['topic'] = df['query'].astype(str).str[0]\n",
    "df_topic = df.groupby(['topic', 'rank', 'method', 'model', 'metric'])[['P', 'CP', 'P-CP']].mean()\n",
    "\n",
    "path_per_topic = os.path.join(results_path, 'res_per_topic' + '.csv')\n",
    "df_topic.to_csv(path_per_topic, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged over all topics\n",
    "df_averaged = df.groupby(['rank', 'method', 'model', 'metric'])[['P', 'CP', 'P-CP']].mean()\n",
    "\n",
    "path_averaged = os.path.join(results_path, 'res_averaged' + '.csv')\n",
    "df_averaged.to_csv(path_averaged, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
